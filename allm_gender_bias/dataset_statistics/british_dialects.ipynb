{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import librosa\n",
    "\n",
    "storage_path = '/work/bbc6523/diverse_voices/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs = {}\n",
    "\n",
    "for file in os.listdir(storage_path + 'british_dialects_audio'):\n",
    "    dfs[file] = pd.read_csv(storage_path + file + '.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_audio_duration(row):\n",
    "    audio, sr = librosa.load(storage_path + 'british_dialects_audio/' + file + '/'+ row['audio_file'] + \".wav\", sr=None)  # sr=None to keep original sample rate\n",
    "\n",
    "    # Duration in seconds\n",
    "    duration = len(audio) / sr\n",
    "    return duration\n",
    "\n",
    "for file in dfs: \n",
    "    dfs[file]['audio_duration'] = dfs[file].apply(lambda row: add_audio_duration(row), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "british_dialects_southern_female\n",
      "4161\n",
      "27\n",
      "2\n",
      "2988\n",
      "16.469333333333335\n",
      "1.6213333333333333\n",
      "1312\n",
      "BRING TOGETHER\n",
      "1306\n",
      "------------------------------------------\n",
      "\n",
      "british_dialects_northern_male\n",
      "2097\n",
      "27\n",
      "2\n",
      "1474\n",
      "14.933333333333334\n",
      "1.6213333333333333\n",
      "651\n",
      "BRING TOGETHER\n",
      "641\n",
      "------------------------------------------\n",
      "\n",
      "british_dialects_southern_welsh_male\n",
      "134\n",
      "26\n",
      "10\n",
      "132\n",
      "23.162666666666667\n",
      "11.130666666666666\n",
      "134\n",
      "BRING TOGETHER\n",
      "134\n",
      "------------------------------------------\n",
      "\n",
      "british_dialects_northern_female\n",
      "750\n",
      "27\n",
      "2\n",
      "521\n",
      "14.677333333333333\n",
      "1.7066666666666668\n",
      "282\n",
      "BRING TOGETHER\n",
      "281\n",
      "------------------------------------------\n",
      "\n",
      "british_dialects_southern_welsh_female\n",
      "98\n",
      "26\n",
      "8\n",
      "92\n",
      "23.674666666666667\n",
      "12.666666666666666\n",
      "98\n",
      "BRING TOGETHER\n",
      "94\n",
      "------------------------------------------\n",
      "\n",
      "british_dialects_southern_midlands_male\n",
      "134\n",
      "26\n",
      "10\n",
      "132\n",
      "22.48\n",
      "10.704\n",
      "134\n",
      "BRING TOGETHER\n",
      "134\n",
      "------------------------------------------\n",
      "\n",
      "british_dialects_southern_northern_male\n",
      "134\n",
      "26\n",
      "10\n",
      "132\n",
      "23.248\n",
      "10.96\n",
      "134\n",
      "BRING TOGETHER\n",
      "134\n",
      "------------------------------------------\n",
      "\n",
      "british_dialects_midlands_female\n",
      "246\n",
      "26\n",
      "4\n",
      "180\n",
      "12.544\n",
      "2.7306666666666666\n",
      "108\n",
      "BRING TOGETHER\n",
      "108\n",
      "------------------------------------------\n",
      "\n",
      "british_dialects_welsh_female\n",
      "1199\n",
      "26\n",
      "2\n",
      "820\n",
      "20.138666666666666\n",
      "1.792\n",
      "609\n",
      "BRING TOGETHER\n",
      "590\n",
      "------------------------------------------\n",
      "\n",
      "british_dialects_scottish_female\n",
      "894\n",
      "27\n",
      "2\n",
      "619\n",
      "13.312\n",
      "1.6213333333333333\n",
      "322\n",
      "BRING TOGETHER\n",
      "321\n",
      "------------------------------------------\n",
      "\n",
      "british_dialects_southern_scottish_male\n",
      "134\n",
      "26\n",
      "10\n",
      "132\n",
      "23.248\n",
      "11.386666666666667\n",
      "134\n",
      "BRING TOGETHER\n",
      "134\n",
      "------------------------------------------\n",
      "\n",
      "british_dialects_southern_male\n",
      "4331\n",
      "27\n",
      "2\n",
      "3088\n",
      "15.701333333333332\n",
      "1.6213333333333333\n",
      "1333\n",
      "BRING TOGETHER\n",
      "1321\n",
      "------------------------------------------\n",
      "\n",
      "british_dialects_southern_scottish_female\n",
      "98\n",
      "26\n",
      "8\n",
      "92\n",
      "22.82133333333333\n",
      "11.130666666666666\n",
      "98\n",
      "BRING TOGETHER\n",
      "94\n",
      "------------------------------------------\n",
      "\n",
      "british_dialects_welsh_male\n",
      "1650\n",
      "26\n",
      "2\n",
      "1119\n",
      "18.432\n",
      "1.792\n",
      "589\n",
      "BRING TOGETHER\n",
      "586\n",
      "------------------------------------------\n",
      "\n",
      "british_dialects_scottish_male\n",
      "1649\n",
      "26\n",
      "2\n",
      "1120\n",
      "12.544\n",
      "2.048\n",
      "473\n",
      "BRING TOGETHER\n",
      "473\n",
      "------------------------------------------\n",
      "\n",
      "british_dialects_southern_midlands_female\n",
      "98\n",
      "26\n",
      "8\n",
      "92\n",
      "22.992\n",
      "10.96\n",
      "98\n",
      "BRING TOGETHER\n",
      "94\n",
      "------------------------------------------\n",
      "\n",
      "british_dialects_southern_northern_female\n",
      "98\n",
      "26\n",
      "8\n",
      "92\n",
      "23.418666666666667\n",
      "10.789333333333333\n",
      "98\n",
      "BRING TOGETHER\n",
      "94\n",
      "------------------------------------------\n",
      "\n",
      "british_dialects_midlands_male\n",
      "450\n",
      "26\n",
      "2\n",
      "327\n",
      "11.946666666666667\n",
      "1.9626666666666666\n",
      "119\n",
      "BRING TOGETHER\n",
      "119\n",
      "------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "subset_dfs = {}\n",
    "for file in dfs:\n",
    "    print(file)\n",
    "    print(len(dfs[file]))\n",
    "    dfs[file]['text_length'] = dfs[file]['text'].str.split().str.len()\n",
    "    print(dfs[file]['text_length'].max())\n",
    "    print(dfs[file]['text_length'].min())\n",
    "    print(len(dfs[file][dfs[file]['text_length'] > 10]))\n",
    "    print(dfs[file]['audio_duration'].max())\n",
    "    print(dfs[file]['audio_duration'].min())\n",
    "    print(len(dfs[file][dfs[file]['audio_duration'] > 7]))\n",
    "    print('BRING TOGETHER')\n",
    "    print(len((dfs[file][(dfs[file]['audio_duration'] >= 7) & (dfs[file]['text_length'] >= 10)])))\n",
    "    #subset_dfs[file] = dfs[file][(dfs[file]['audio_duration'] >= 5) & (dfs[file]['text_length'] >= 5)]\n",
    "    subset_dfs[file] = dfs[file][(dfs[file]['audio_duration'] >=5)]\n",
    "    print('------------------------------------------')\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_list_male = [\n",
    "'british_dialects_northern_male',\n",
    "'british_dialects_southern_male',\n",
    "'british_dialects_welsh_male',\n",
    "'british_dialects_scottish_male',\n",
    "'british_dialects_midlands_male']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_list_female = ['british_dialects_southern_female',\n",
    "'british_dialects_northern_female',\n",
    "'british_dialects_midlands_female',\n",
    "'british_dialects_welsh_female',\n",
    "'british_dialects_scottish_female',]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_list_male_female = ['british_dialects_southern_female', 'british_dialects_southern_male']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1083"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "value_sets = [set(subset_dfs[file]['text']) for file in file_list_male_female]\n",
    "common_values_female = set.intersection(*value_sets)\n",
    "len(common_values_female)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "67"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "value_sets = [set(subset_dfs[file]['text']) for file in file_list_male]\n",
    "common_values_male = set.intersection(*value_sets)\n",
    "len(common_values_male)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "49"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "value_sets = [set(subset_dfs[file]['text']) for file in file_list_female]\n",
    "common_values_female = set.intersection(*value_sets)\n",
    "len(common_values_female)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2581163/1161927865.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  subset_dfs[file]['__index'] = subset_dfs[file].index\n",
      "/tmp/ipykernel_2581163/1161927865.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  subset_dfs[file]['__index'] = subset_dfs[file].index\n",
      "/tmp/ipykernel_2581163/1161927865.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  subset_dfs[file]['__index'] = subset_dfs[file].index\n",
      "/tmp/ipykernel_2581163/1161927865.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  subset_dfs[file]['__index'] = subset_dfs[file].index\n",
      "/tmp/ipykernel_2581163/1161927865.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  subset_dfs[file]['__index'] = subset_dfs[file].index\n",
      "/tmp/ipykernel_2581163/1161927865.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  subset_dfs[file]['__index'] = subset_dfs[file].index\n",
      "/tmp/ipykernel_2581163/1161927865.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  subset_dfs[file]['__index'] = subset_dfs[file].index\n",
      "/tmp/ipykernel_2581163/1161927865.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  subset_dfs[file]['__index'] = subset_dfs[file].index\n",
      "/tmp/ipykernel_2581163/1161927865.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  subset_dfs[file]['__index'] = subset_dfs[file].index\n",
      "/tmp/ipykernel_2581163/1161927865.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  subset_dfs[file]['__index'] = subset_dfs[file].index\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "for file in subset_dfs: \n",
    "    subset_dfs[file]['__index'] = subset_dfs[file].index\n",
    "\n",
    "grouped = {\n",
    "    name: df.groupby(['text_length', 'audio_duration_rounded'])['__index'].apply(list)\n",
    "    for name, df in subset_dfs.items()\n",
    "}\n",
    "\n",
    "# Step 3: Find shared keys (only those that exist in all DataFrames)\n",
    "shared_keys = set.intersection(*(set(g.index) for g in grouped.values()))\n",
    "\n",
    "# Step 4: Build matchable aligned rows across all DataFrames\n",
    "matches = []\n",
    "for key in shared_keys:\n",
    "    min_count = min(len(grouped[name][key]) for name in grouped)\n",
    "    for i in range(min_count):\n",
    "        match = [grouped[name][key][i] for name in subset_dfs]\n",
    "        matches.append(match)\n",
    "\n",
    "if len(matches) < 100:\n",
    "    raise ValueError(f\"Only {len(matches)} matched rows available, can't sample 100.\")\n",
    "\n",
    "sampled_matches = random.sample(matches, 100)\n",
    "\n",
    "# Step 6: Build filtered versions of the DataFrames\n",
    "filtered_subset_dfs = {}\n",
    "for df_idx, name in enumerate(subset_dfs):\n",
    "    matched_indices = [match[df_idx] for match in sampled_matches]\n",
    "    filtered_subset_dfs[name] = subset_dfs[name].loc[matched_indices].reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "british_dialects_southern_female\n",
      "100\n",
      "british_dialects_northern_male\n",
      "100\n",
      "british_dialects_northern_female\n",
      "100\n",
      "british_dialects_midlands_female\n",
      "100\n",
      "british_dialects_welsh_female\n",
      "100\n",
      "british_dialects_scottish_female\n",
      "100\n",
      "british_dialects_southern_male\n",
      "100\n",
      "british_dialects_welsh_male\n",
      "100\n",
      "british_dialects_scottish_male\n",
      "100\n",
      "british_dialects_midlands_male\n",
      "100\n"
     ]
    }
   ],
   "source": [
    "for file in filtered_subset_dfs:\n",
    "    print(file)\n",
    "    print(len(filtered_subset_dfs[file]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import soundfile as sf\n",
    "import time\n",
    "\n",
    "def create_merged_file(row, standard_file_name, dialect_file_name):\n",
    "    standard_audio, sr1 = librosa.load(storage_path + 'british_dialects_audio/' + standard_file_name + '/'+ row['audio_file_standard'] + \".wav\", sr=None)\n",
    "    dialect_audio, sr2 = librosa.load(storage_path + 'british_dialects_audio/' + dialect_file_name + '/'+ row['audio_file_dialect'] + \".wav\", sr=None)\n",
    "\n",
    "    # Create 2 seconds of silence\n",
    "    pause_duration_sec = 2\n",
    "    pause = np.zeros(int(sr1 * pause_duration_sec))\n",
    "    if row['dialect_order'] == 'standard-dialect':\n",
    "        joined = np.concatenate([standard_audio, pause, dialect_audio])\n",
    "        file_name = str(row['index']) +'_'+ 'british_dialects_' + str(standard_file_name.replace('british_dialects_', '').replace('_male', '')) + '_' + str(dialect_file_name.replace('british_dialects_', '').replace('_male', ''))+ '_male'\n",
    "    if row['dialect_order'] == 'dialect-standard':\n",
    "        joined = np.concatenate([dialect_audio, pause, standard_audio])\n",
    "        file_name = str(row['index'])+'_' + 'british_dialects_' + str(dialect_file_name.replace('british_dialects_', '').replace('_male', '')) + '_' + str(standard_file_name.replace('british_dialects_', '').replace('_male', ''))+ '_male'\n",
    "\n",
    "    storage = storage_path +'british_dialects_audio/compare_audio/' + file_name + '.wav'\n",
    "    os.makedirs(storage_path +'british_dialects_audio/compare_audio' , exist_ok=True)\n",
    "\n",
    "    try:\n",
    "        sf.write(storage, joined, sr1)\n",
    "    except: \n",
    "        print(file_name)\n",
    "\n",
    "    return file_name\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "67\n",
      "67\n"
     ]
    }
   ],
   "source": [
    "base_file_male = 'british_dialects_southern_male'\n",
    "\n",
    "for dialect_file_male in ['british_dialects_welsh_male']:\n",
    "                         # 'british_dialects_midlands_male'\n",
    "                        #'british_dialects_northern_male',\n",
    "                        #'british_dialects_scottish_male',\n",
    "                        #'british_dialects_welsh_male']:\n",
    "\n",
    "   df1 = dfs[base_file_male].copy()\n",
    "   df1 = df1[df1['text'].isin(list(common_values_male))]\n",
    "   df1 = df1[df1['audio_duration'] < 11]\n",
    "   df1 = df1.drop_duplicates(subset='text')\n",
    "   print(len(df1))\n",
    "   df2 = dfs[dialect_file_male].copy()\n",
    "   df2 = df2[df2['text'].isin(list(common_values_male))]\n",
    "   df2 = df2[df2['audio_duration'] < 11]\n",
    "   df2 = df2.drop_duplicates(subset='text')\n",
    "   print(len(df2))\n",
    "\n",
    "\n",
    "   df1 = df1[['line_id', 'audio_file', 'text', 'text_length', 'audio_duration']]\n",
    "   df2 = df2[['line_id', 'audio_file', 'text', 'text_length', 'audio_duration']]\n",
    "\n",
    "   # Merge using this row ID\n",
    "   merged = pd.merge(df1, df2, on='text', suffixes=('_standard', '_dialect'))\n",
    "\n",
    "   merged['standard_file'] = base_file_male\n",
    "   merged['dialect_file'] = dialect_file_male\n",
    "   merged = merged.reset_index()\n",
    "\n",
    "   merged['dialect_order'] = 'standard-dialect'\n",
    "   merged_2 = merged.copy()\n",
    "\n",
    "   merged_2['dialect_order'] = 'dialect-standard'\n",
    "\n",
    "   full_df = pd.concat([merged, merged_2])\n",
    "\n",
    "   full_df['audio_file'] = full_df.apply(lambda row: create_merged_file(row, base_file_male, dialect_file_male), axis=1)\n",
    "\n",
    "\n",
    "   df_name = 'british_dialects_' + base_file_male.replace('british_dialects_', '').replace('_male', '') + '_'+ dialect_file_male.replace('british_dialects_', '').replace('_male', '') + '_male.csv'\n",
    "   full_df.to_csv('merged_audio/' + df_name, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>line_id</th>\n",
       "      <th>audio_file</th>\n",
       "      <th>text</th>\n",
       "      <th>text_length</th>\n",
       "      <th>audio_duration</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>EN0021</td>\n",
       "      <td>som_00295_00472546698</td>\n",
       "      <td>This is a very common type of bow one showing ...</td>\n",
       "      <td>21</td>\n",
       "      <td>6.314667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>192</th>\n",
       "      <td>EN0021</td>\n",
       "      <td>som_04310_01192796020</td>\n",
       "      <td>This is a very common type of bow one showing ...</td>\n",
       "      <td>21</td>\n",
       "      <td>8.192000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>892</th>\n",
       "      <td>EN0021</td>\n",
       "      <td>som_05223_01459906245</td>\n",
       "      <td>This is a very common type of bow one showing ...</td>\n",
       "      <td>21</td>\n",
       "      <td>7.082667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>970</th>\n",
       "      <td>EN0021</td>\n",
       "      <td>som_02121_00801090069</td>\n",
       "      <td>This is a very common type of bow one showing ...</td>\n",
       "      <td>21</td>\n",
       "      <td>7.338667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1270</th>\n",
       "      <td>EN0021</td>\n",
       "      <td>som_08886_00800439310</td>\n",
       "      <td>This is a very common type of bow one showing ...</td>\n",
       "      <td>21</td>\n",
       "      <td>6.570667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1330</th>\n",
       "      <td>EN0021</td>\n",
       "      <td>som_03034_00601255200</td>\n",
       "      <td>This is a very common type of bow one showing ...</td>\n",
       "      <td>21</td>\n",
       "      <td>8.362667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1611</th>\n",
       "      <td>EN0021</td>\n",
       "      <td>som_01523_00877829907</td>\n",
       "      <td>This is a very common type of bow one showing ...</td>\n",
       "      <td>21</td>\n",
       "      <td>6.058667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1627</th>\n",
       "      <td>EN0021</td>\n",
       "      <td>som_06136_00960050331</td>\n",
       "      <td>This is a very common type of bow one showing ...</td>\n",
       "      <td>21</td>\n",
       "      <td>7.850667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2081</th>\n",
       "      <td>EN0021</td>\n",
       "      <td>som_09799_01193920807</td>\n",
       "      <td>This is a very common type of bow one showing ...</td>\n",
       "      <td>21</td>\n",
       "      <td>6.741333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2392</th>\n",
       "      <td>EN0021</td>\n",
       "      <td>som_09697_02085046691</td>\n",
       "      <td>This is a very common type of bow one showing ...</td>\n",
       "      <td>21</td>\n",
       "      <td>6.912000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2790</th>\n",
       "      <td>EN0021</td>\n",
       "      <td>som_07060_01435775257</td>\n",
       "      <td>This is a very common type of bow one showing ...</td>\n",
       "      <td>21</td>\n",
       "      <td>6.997333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2802</th>\n",
       "      <td>EN0021</td>\n",
       "      <td>som_09334_00555588387</td>\n",
       "      <td>This is a very common type of bow one showing ...</td>\n",
       "      <td>21</td>\n",
       "      <td>6.058667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2843</th>\n",
       "      <td>EN0021</td>\n",
       "      <td>som_07049_01939522664</td>\n",
       "      <td>This is a very common type of bow one showing ...</td>\n",
       "      <td>21</td>\n",
       "      <td>7.680000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2866</th>\n",
       "      <td>EN0021</td>\n",
       "      <td>som_07508_00183758006</td>\n",
       "      <td>This is a very common type of bow one showing ...</td>\n",
       "      <td>21</td>\n",
       "      <td>7.509333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2875</th>\n",
       "      <td>EN0021</td>\n",
       "      <td>som_05679_02110303353</td>\n",
       "      <td>This is a very common type of bow one showing ...</td>\n",
       "      <td>21</td>\n",
       "      <td>7.082667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2896</th>\n",
       "      <td>EN0021</td>\n",
       "      <td>som_00610_01828452424</td>\n",
       "      <td>This is a very common type of bow one showing ...</td>\n",
       "      <td>21</td>\n",
       "      <td>7.594667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2911</th>\n",
       "      <td>EN0021</td>\n",
       "      <td>som_02436_00063118805</td>\n",
       "      <td>This is a very common type of bow one showing ...</td>\n",
       "      <td>21</td>\n",
       "      <td>8.789333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2915</th>\n",
       "      <td>EN0021</td>\n",
       "      <td>som_03853_01713682305</td>\n",
       "      <td>This is a very common type of bow one showing ...</td>\n",
       "      <td>21</td>\n",
       "      <td>8.618667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3060</th>\n",
       "      <td>EN0021</td>\n",
       "      <td>som_01208_00926893421</td>\n",
       "      <td>This is a very common type of bow one showing ...</td>\n",
       "      <td>21</td>\n",
       "      <td>8.021333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3215</th>\n",
       "      <td>EN0021</td>\n",
       "      <td>som_00712_01801130561</td>\n",
       "      <td>This is a very common type of bow one showing ...</td>\n",
       "      <td>21</td>\n",
       "      <td>7.168000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3218</th>\n",
       "      <td>EN0021</td>\n",
       "      <td>som_08421_01733516770</td>\n",
       "      <td>This is a very common type of bow one showing ...</td>\n",
       "      <td>21</td>\n",
       "      <td>7.338667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3301</th>\n",
       "      <td>EN0021</td>\n",
       "      <td>som_08784_01161692414</td>\n",
       "      <td>This is a very common type of bow one showing ...</td>\n",
       "      <td>21</td>\n",
       "      <td>9.216000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3354</th>\n",
       "      <td>EN0021</td>\n",
       "      <td>som_07505_00348244867</td>\n",
       "      <td>This is a very common type of bow one showing ...</td>\n",
       "      <td>21</td>\n",
       "      <td>8.362667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3484</th>\n",
       "      <td>EN0021</td>\n",
       "      <td>som_03349_01697303866</td>\n",
       "      <td>This is a very common type of bow one showing ...</td>\n",
       "      <td>21</td>\n",
       "      <td>6.826667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3509</th>\n",
       "      <td>EN0021</td>\n",
       "      <td>som_06592_01934585757</td>\n",
       "      <td>This is a very common type of bow one showing ...</td>\n",
       "      <td>21</td>\n",
       "      <td>8.021333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3836</th>\n",
       "      <td>EN0021</td>\n",
       "      <td>som_04766_00731492379</td>\n",
       "      <td>This is a very common type of bow one showing ...</td>\n",
       "      <td>21</td>\n",
       "      <td>8.021333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4097</th>\n",
       "      <td>EN0021</td>\n",
       "      <td>som_03397_02028297909</td>\n",
       "      <td>This is a very common type of bow one showing ...</td>\n",
       "      <td>21</td>\n",
       "      <td>7.168000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4241</th>\n",
       "      <td>EN0021</td>\n",
       "      <td>som_02484_00933955681</td>\n",
       "      <td>This is a very common type of bow one showing ...</td>\n",
       "      <td>21</td>\n",
       "      <td>9.130667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4271</th>\n",
       "      <td>EN0021</td>\n",
       "      <td>som_03502_01343713455</td>\n",
       "      <td>This is a very common type of bow one showing ...</td>\n",
       "      <td>21</td>\n",
       "      <td>9.557333</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     line_id             audio_file  \\\n",
       "1     EN0021  som_00295_00472546698   \n",
       "192   EN0021  som_04310_01192796020   \n",
       "892   EN0021  som_05223_01459906245   \n",
       "970   EN0021  som_02121_00801090069   \n",
       "1270  EN0021  som_08886_00800439310   \n",
       "1330  EN0021  som_03034_00601255200   \n",
       "1611  EN0021  som_01523_00877829907   \n",
       "1627  EN0021  som_06136_00960050331   \n",
       "2081  EN0021  som_09799_01193920807   \n",
       "2392  EN0021  som_09697_02085046691   \n",
       "2790  EN0021  som_07060_01435775257   \n",
       "2802  EN0021  som_09334_00555588387   \n",
       "2843  EN0021  som_07049_01939522664   \n",
       "2866  EN0021  som_07508_00183758006   \n",
       "2875  EN0021  som_05679_02110303353   \n",
       "2896  EN0021  som_00610_01828452424   \n",
       "2911  EN0021  som_02436_00063118805   \n",
       "2915  EN0021  som_03853_01713682305   \n",
       "3060  EN0021  som_01208_00926893421   \n",
       "3215  EN0021  som_00712_01801130561   \n",
       "3218  EN0021  som_08421_01733516770   \n",
       "3301  EN0021  som_08784_01161692414   \n",
       "3354  EN0021  som_07505_00348244867   \n",
       "3484  EN0021  som_03349_01697303866   \n",
       "3509  EN0021  som_06592_01934585757   \n",
       "3836  EN0021  som_04766_00731492379   \n",
       "4097  EN0021  som_03397_02028297909   \n",
       "4241  EN0021  som_02484_00933955681   \n",
       "4271  EN0021  som_03502_01343713455   \n",
       "\n",
       "                                                   text  text_length  \\\n",
       "1     This is a very common type of bow one showing ...           21   \n",
       "192   This is a very common type of bow one showing ...           21   \n",
       "892   This is a very common type of bow one showing ...           21   \n",
       "970   This is a very common type of bow one showing ...           21   \n",
       "1270  This is a very common type of bow one showing ...           21   \n",
       "1330  This is a very common type of bow one showing ...           21   \n",
       "1611  This is a very common type of bow one showing ...           21   \n",
       "1627  This is a very common type of bow one showing ...           21   \n",
       "2081  This is a very common type of bow one showing ...           21   \n",
       "2392  This is a very common type of bow one showing ...           21   \n",
       "2790  This is a very common type of bow one showing ...           21   \n",
       "2802  This is a very common type of bow one showing ...           21   \n",
       "2843  This is a very common type of bow one showing ...           21   \n",
       "2866  This is a very common type of bow one showing ...           21   \n",
       "2875  This is a very common type of bow one showing ...           21   \n",
       "2896  This is a very common type of bow one showing ...           21   \n",
       "2911  This is a very common type of bow one showing ...           21   \n",
       "2915  This is a very common type of bow one showing ...           21   \n",
       "3060  This is a very common type of bow one showing ...           21   \n",
       "3215  This is a very common type of bow one showing ...           21   \n",
       "3218  This is a very common type of bow one showing ...           21   \n",
       "3301  This is a very common type of bow one showing ...           21   \n",
       "3354  This is a very common type of bow one showing ...           21   \n",
       "3484  This is a very common type of bow one showing ...           21   \n",
       "3509  This is a very common type of bow one showing ...           21   \n",
       "3836  This is a very common type of bow one showing ...           21   \n",
       "4097  This is a very common type of bow one showing ...           21   \n",
       "4241  This is a very common type of bow one showing ...           21   \n",
       "4271  This is a very common type of bow one showing ...           21   \n",
       "\n",
       "      audio_duration  \n",
       "1           6.314667  \n",
       "192         8.192000  \n",
       "892         7.082667  \n",
       "970         7.338667  \n",
       "1270        6.570667  \n",
       "1330        8.362667  \n",
       "1611        6.058667  \n",
       "1627        7.850667  \n",
       "2081        6.741333  \n",
       "2392        6.912000  \n",
       "2790        6.997333  \n",
       "2802        6.058667  \n",
       "2843        7.680000  \n",
       "2866        7.509333  \n",
       "2875        7.082667  \n",
       "2896        7.594667  \n",
       "2911        8.789333  \n",
       "2915        8.618667  \n",
       "3060        8.021333  \n",
       "3215        7.168000  \n",
       "3218        7.338667  \n",
       "3301        9.216000  \n",
       "3354        8.362667  \n",
       "3484        6.826667  \n",
       "3509        8.021333  \n",
       "3836        8.021333  \n",
       "4097        7.168000  \n",
       "4241        9.130667  \n",
       "4271        9.557333  "
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1[df1['line_id'] == 'EN0021']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "49\n",
      "49\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import soundfile as sf\n",
    "import time\n",
    "\n",
    "def create_merged_file(row, standard_file_name, dialect_file_name):\n",
    "    standard_audio, sr1 = librosa.load(storage_path + 'british_dialects_audio/' + standard_file_name + '/'+ row['audio_file_standard'] + \".wav\", sr=None)\n",
    "    dialect_audio, sr2 = librosa.load(storage_path + 'british_dialects_audio/' + dialect_file_name + '/'+ row['audio_file_dialect'] + \".wav\", sr=None)\n",
    "\n",
    "    # Create 2 seconds of silence\n",
    "    pause_duration_sec = 2\n",
    "    pause = np.zeros(int(sr1 * pause_duration_sec))\n",
    "    if row['dialect_order'] == 'standard-dialect':\n",
    "        joined = np.concatenate([standard_audio, pause, dialect_audio])\n",
    "        file_name = str(row['index']) +'_'+ 'british_dialects_' + str(standard_file_name.replace('british_dialects_', '').replace('_female', '')) + '_' + str(dialect_file_name.replace('british_dialects_', '').replace('_female', ''))+ '_female'\n",
    "    if row['dialect_order'] == 'dialect-standard':\n",
    "        joined = np.concatenate([dialect_audio, pause, standard_audio])\n",
    "        file_name = str(row['index'])+'_' + 'british_dialects_' + str(dialect_file_name.replace('british_dialects_', '').replace('_female', '')) + '_' + str(standard_file_name.replace('british_dialects_', '').replace('_female', ''))+ '_female'\n",
    "\n",
    "    storage = storage_path +'british_dialects_audio/compare_audio/' + file_name + '.wav'\n",
    "    os.makedirs(storage_path +'british_dialects_audio/compare_audio' , exist_ok=True)\n",
    "\n",
    "    try:\n",
    "        sf.write(storage, joined, sr1)\n",
    "    except: \n",
    "        print(file_name)\n",
    "\n",
    "    return file_name\n",
    "\n",
    "\n",
    "base_file_female = 'british_dialects_southern_female'\n",
    "\n",
    "for dialect_file_female in ['british_dialects_scottish_female']:\n",
    "                         # 'british_dialects_midlands_female'\n",
    "                        #'british_dialects_northern_female',\n",
    "                        #'british_dialects_scottish_female',\n",
    "                        #'british_dialects_welsh_female']:\n",
    "\n",
    "   df1 = dfs[base_file_female].copy()\n",
    "   df1 = df1[df1['text'].isin(list(common_values_female))]\n",
    "   df1 = df1[df1['audio_duration'] < 11]\n",
    "   df1 = df1.drop_duplicates(subset='text')\n",
    "   print(len(df1))\n",
    "   df2 = dfs[dialect_file_female].copy()\n",
    "   df2 = df2[df2['text'].isin(list(common_values_female))]\n",
    "   df2 = df2[df2['audio_duration'] < 11]\n",
    "   df2 = df2.drop_duplicates(subset='text')\n",
    "   print(len(df2))\n",
    "\n",
    "\n",
    "   df1 = df1[['line_id', 'audio_file', 'text', 'text_length', 'audio_duration']]\n",
    "   df2 = df2[['line_id', 'audio_file', 'text', 'text_length', 'audio_duration']]\n",
    "\n",
    "   # Merge using this row ID\n",
    "   merged = pd.merge(df1, df2, on='text', suffixes=('_standard', '_dialect'))\n",
    "\n",
    "   merged['standard_file'] = base_file_female\n",
    "   merged['dialect_file'] = dialect_file_female\n",
    "   merged = merged.reset_index()\n",
    "\n",
    "   merged['dialect_order'] = 'standard-dialect'\n",
    "   merged_2 = merged.copy()\n",
    "\n",
    "   merged_2['dialect_order'] = 'dialect-standard'\n",
    "\n",
    "   full_df = pd.concat([merged, merged_2])\n",
    "\n",
    "   full_df['audio_file'] = full_df.apply(lambda row: create_merged_file(row, base_file_female, dialect_file_female), axis=1)\n",
    "\n",
    "\n",
    "   df_name = 'british_dialects_' + base_file_female.replace('british_dialects_', '').replace('_female', '') + '_'+ dialect_file_female.replace('british_dialects_', '').replace('_female', '') + '_female.csv'\n",
    "   full_df.to_csv('merged_audio/' + df_name, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'subset' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43msubset\u001b[49m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'subset' is not defined"
     ]
    }
   ],
   "source": [
    "subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "101\n",
      "1071\n",
      "100\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import soundfile as sf\n",
    "import time\n",
    "\n",
    "def create_merged_file_fm(row, base_file_female, base_file_male):\n",
    "    female_audio, sr1 = librosa.load(storage_path + 'british_dialects_audio/' + base_file_female + '/'+ row['audio_file_female'] + \".wav\", sr=None)\n",
    "    male_audio, sr2 = librosa.load(storage_path + 'british_dialects_audio/' + base_file_male + '/'+ row['audio_file_male'] + \".wav\", sr=None)\n",
    "\n",
    "    # Create 2 seconds of silence\n",
    "    pause_duration_sec = 2\n",
    "    pause = np.zeros(int(sr1 * pause_duration_sec))\n",
    "    if row['gender_order'] == 'female-male':\n",
    "        joined = np.concatenate([female_audio, pause, male_audio])\n",
    "        file_name = str(row['index']) +'_'+ 'british_dialects_southern_fm'\n",
    "    if row['gender_order'] == 'male-female':\n",
    "        joined = np.concatenate([male_audio, pause, female_audio])\n",
    "        file_name = str(row['index'])+'_' + 'british_dialects_southern_mf'\n",
    "\n",
    "    storage = storage_path +'british_dialects_audio/compare_audio/' + file_name + '.wav'\n",
    "    os.makedirs(storage_path +'british_dialects_audio/compare_audio' , exist_ok=True)\n",
    "\n",
    "    try:\n",
    "        sf.write(storage, joined, sr1)\n",
    "    except: \n",
    "        print(file_name)\n",
    "\n",
    "    return file_name\n",
    "\n",
    "\n",
    "base_file_female = 'british_dialects_southern_female'\n",
    "\n",
    "base_file_male = 'british_dialects_southern_male'\n",
    "\n",
    "\n",
    "value_sets = [set(subset_dfs[file]['text']) for file in file_list_male_female]\n",
    "common_values_female = set.intersection(*value_sets)\n",
    "len(common_values_female)\n",
    "\n",
    "df1 = dfs[base_file_female].copy()\n",
    "df1 = df1[df1['text'].isin(list(common_values_female))]\n",
    "df1 = df1[df1['audio_duration'] >= 5]\n",
    "df1 = df1[df1['text_length'] >= 5]\n",
    "df1 = df1.drop_duplicates(subset='text')\n",
    "df1 = df1.drop_duplicates(subset='line_id')\n",
    "df1 = df1.sample(n=101, random_state=1)\n",
    "print(len(df1))\n",
    "df2 = dfs[base_file_male].copy()\n",
    "df2 = df2[df2['text'].isin(list(common_values_female))]\n",
    "df2 = df2[df2['audio_duration'] >= 5]\n",
    "df2 = df2[df2['text_length'] >= 5]\n",
    "df2 = df2.drop_duplicates(subset='text')\n",
    "df2 = df2.drop_duplicates(subset='line_id')\n",
    "print(len(df2))\n",
    "\n",
    "df1 = df1[['line_id', 'audio_file', 'text', 'text_length', 'audio_duration']]\n",
    "df2 = df2[['line_id', 'audio_file', 'text', 'text_length', 'audio_duration']]\n",
    "\n",
    "# Merge using this row ID\n",
    "merged = pd.merge(df1, df2, on='text', suffixes=('_female', '_male'))\n",
    "print(len(merged))\n",
    "\n",
    "merged['female_file'] = base_file_female\n",
    "merged['male_file'] = base_file_male\n",
    "\n",
    "merged = merged.reset_index()\n",
    "\n",
    "merged['gender_order'] = 'female-male'\n",
    "\n",
    "merged_2 = merged.copy()\n",
    "\n",
    "merged_2['gender_order'] = 'male-female'\n",
    "\n",
    "full_df = pd.concat([merged, merged_2])\n",
    "\n",
    "full_df['audio_file'] = full_df.apply(lambda row: create_merged_file_fm(row, base_file_female, base_file_male), axis=1)\n",
    "\n",
    "\n",
    "df_name = 'british_dialects_southern_gender.csv'\n",
    "full_df.to_csv('merged_audio/' + df_name, index=False)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1071"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df2.line_id.unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filter dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "storage_path = '/work/bbc6523/diverse_voices/'\n",
    "\n",
    "files = ['british_dialects_scottish_male.csv',\n",
    " 'british_dialects_midlands_male.csv',\n",
    " 'british_dialects_midlands_female.csv',\n",
    " 'british_dialects_welsh_male.csv',\n",
    " 'british_dialects_southern_female.csv',\n",
    " 'british_dialects_southern_male.csv',\n",
    " 'british_dialects_scottish_female.csv',\n",
    " 'british_dialects_northern_male.csv',\n",
    " 'british_dialects_northern_female.csv',\n",
    " 'british_dialects_welsh_female.csv']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs = {}\n",
    "for file in files:\n",
    "    dfs[file] = pd.read_csv(storage_path + 'data/' + file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_audio_duration(row):\n",
    "    audio, sr = librosa.load(storage_path + 'british_dialects_audio/' + file[:-4] + '/'+ row['audio_file'] + \".wav\", sr=None)  # sr=None to keep original sample rate\n",
    "\n",
    "    # Duration in seconds\n",
    "    duration = len(audio) / sr\n",
    "    return duration\n",
    "\n",
    "for file in dfs: \n",
    "    dfs[file]['audio_duration'] = dfs[file].apply(lambda row: add_audio_duration(row), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "british_dialects_scottish_male.csv\n",
      "british_dialects_midlands_male.csv\n",
      "british_dialects_midlands_female.csv\n",
      "british_dialects_welsh_male.csv\n",
      "british_dialects_southern_female.csv\n",
      "british_dialects_southern_male.csv\n",
      "british_dialects_scottish_female.csv\n",
      "british_dialects_northern_male.csv\n",
      "british_dialects_northern_female.csv\n",
      "british_dialects_welsh_female.csv\n"
     ]
    }
   ],
   "source": [
    "subset_dfs = {}\n",
    "for file in dfs:\n",
    "    print(file)\n",
    "    subset_dfs[file] = dfs[file][(dfs[file]['audio_duration'] >=4)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "value_sets = [set(subset_dfs[file]['text']) for file in files]\n",
    "common_values_text = set.intersection(*value_sets)\n",
    "len(common_values_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "51"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "value_sets = [set(subset_dfs[file]['line_id']) for file in files]\n",
    "common_values = set.intersection(*value_sets)\n",
    "len(common_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3      /work/bbc6523/diverse_voices/british_dialects_audio/british_dialects_welsh_female/wef_12484_00036278823.wav\n",
       "7      /work/bbc6523/diverse_voices/british_dialects_audio/british_dialects_welsh_female/wef_12484_00495324118.wav\n",
       "10     /work/bbc6523/diverse_voices/british_dialects_audio/british_dialects_welsh_female/wef_12484_00407944176.wav\n",
       "12     /work/bbc6523/diverse_voices/british_dialects_audio/british_dialects_welsh_female/wef_12484_01518530037.wav\n",
       "14     /work/bbc6523/diverse_voices/british_dialects_audio/british_dialects_welsh_female/wef_12484_01289814182.wav\n",
       "15     /work/bbc6523/diverse_voices/british_dialects_audio/british_dialects_welsh_female/wef_12484_00263357569.wav\n",
       "16     /work/bbc6523/diverse_voices/british_dialects_audio/british_dialects_welsh_female/wef_12484_01407056263.wav\n",
       "18     /work/bbc6523/diverse_voices/british_dialects_audio/british_dialects_welsh_female/wef_12484_01739883713.wav\n",
       "24     /work/bbc6523/diverse_voices/british_dialects_audio/british_dialects_welsh_female/wef_12484_00698536202.wav\n",
       "26     /work/bbc6523/diverse_voices/british_dialects_audio/british_dialects_welsh_female/wef_12484_00713158670.wav\n",
       "27     /work/bbc6523/diverse_voices/british_dialects_audio/british_dialects_welsh_female/wef_12484_00587666053.wav\n",
       "32     /work/bbc6523/diverse_voices/british_dialects_audio/british_dialects_welsh_female/wef_12484_01608390610.wav\n",
       "33     /work/bbc6523/diverse_voices/british_dialects_audio/british_dialects_welsh_female/wef_12484_00264636558.wav\n",
       "36     /work/bbc6523/diverse_voices/british_dialects_audio/british_dialects_welsh_female/wef_12484_00161174827.wav\n",
       "42     /work/bbc6523/diverse_voices/british_dialects_audio/british_dialects_welsh_female/wef_12484_00004861543.wav\n",
       "51     /work/bbc6523/diverse_voices/british_dialects_audio/british_dialects_welsh_female/wef_12484_00900564235.wav\n",
       "52     /work/bbc6523/diverse_voices/british_dialects_audio/british_dialects_welsh_female/wef_12484_00267149582.wav\n",
       "53     /work/bbc6523/diverse_voices/british_dialects_audio/british_dialects_welsh_female/wef_12484_01367585884.wav\n",
       "60     /work/bbc6523/diverse_voices/british_dialects_audio/british_dialects_welsh_female/wef_12484_01662909519.wav\n",
       "76     /work/bbc6523/diverse_voices/british_dialects_audio/british_dialects_welsh_female/wef_12484_00396337649.wav\n",
       "77     /work/bbc6523/diverse_voices/british_dialects_audio/british_dialects_welsh_female/wef_12484_00933622642.wav\n",
       "80     /work/bbc6523/diverse_voices/british_dialects_audio/british_dialects_welsh_female/wef_12484_00470680715.wav\n",
       "85     /work/bbc6523/diverse_voices/british_dialects_audio/british_dialects_welsh_female/wef_12484_01714006986.wav\n",
       "86     /work/bbc6523/diverse_voices/british_dialects_audio/british_dialects_welsh_female/wef_12484_01230993698.wav\n",
       "87     /work/bbc6523/diverse_voices/british_dialects_audio/british_dialects_welsh_female/wef_12484_01038372673.wav\n",
       "90     /work/bbc6523/diverse_voices/british_dialects_audio/british_dialects_welsh_female/wef_12484_01819533974.wav\n",
       "93     /work/bbc6523/diverse_voices/british_dialects_audio/british_dialects_welsh_female/wef_12484_01329884078.wav\n",
       "94     /work/bbc6523/diverse_voices/british_dialects_audio/british_dialects_welsh_female/wef_12484_00568811649.wav\n",
       "95     /work/bbc6523/diverse_voices/british_dialects_audio/british_dialects_welsh_female/wef_12484_01164910905.wav\n",
       "98     /work/bbc6523/diverse_voices/british_dialects_audio/british_dialects_welsh_female/wef_12484_00361833646.wav\n",
       "100    /work/bbc6523/diverse_voices/british_dialects_audio/british_dialects_welsh_female/wef_12484_00805037075.wav\n",
       "101    /work/bbc6523/diverse_voices/british_dialects_audio/british_dialects_welsh_female/wef_12484_00870834647.wav\n",
       "113    /work/bbc6523/diverse_voices/british_dialects_audio/british_dialects_welsh_female/wef_12484_00932273538.wav\n",
       "114    /work/bbc6523/diverse_voices/british_dialects_audio/british_dialects_welsh_female/wef_12484_00632024344.wav\n",
       "116    /work/bbc6523/diverse_voices/british_dialects_audio/british_dialects_welsh_female/wef_12484_00287050244.wav\n",
       "122    /work/bbc6523/diverse_voices/british_dialects_audio/british_dialects_welsh_female/wef_12484_01021403361.wav\n",
       "124    /work/bbc6523/diverse_voices/british_dialects_audio/british_dialects_welsh_female/wef_12484_01928424694.wav\n",
       "126    /work/bbc6523/diverse_voices/british_dialects_audio/british_dialects_welsh_female/wef_12484_01849068691.wav\n",
       "127    /work/bbc6523/diverse_voices/british_dialects_audio/british_dialects_welsh_female/wef_12484_00981358533.wav\n",
       "131    /work/bbc6523/diverse_voices/british_dialects_audio/british_dialects_welsh_female/wef_12484_01777283566.wav\n",
       "132    /work/bbc6523/diverse_voices/british_dialects_audio/british_dialects_welsh_female/wef_12484_01738206140.wav\n",
       "134    /work/bbc6523/diverse_voices/british_dialects_audio/british_dialects_welsh_female/wef_12484_01604069402.wav\n",
       "135    /work/bbc6523/diverse_voices/british_dialects_audio/british_dialects_welsh_female/wef_12484_01483038671.wav\n",
       "139    /work/bbc6523/diverse_voices/british_dialects_audio/british_dialects_welsh_female/wef_12484_01793991733.wav\n",
       "141    /work/bbc6523/diverse_voices/british_dialects_audio/british_dialects_welsh_female/wef_12484_00281926524.wav\n",
       "142    /work/bbc6523/diverse_voices/british_dialects_audio/british_dialects_welsh_female/wef_12484_00023750568.wav\n",
       "147    /work/bbc6523/diverse_voices/british_dialects_audio/british_dialects_welsh_female/wef_12484_01892031727.wav\n",
       "149    /work/bbc6523/diverse_voices/british_dialects_audio/british_dialects_welsh_female/wef_12484_02132649775.wav\n",
       "162    /work/bbc6523/diverse_voices/british_dialects_audio/british_dialects_welsh_female/wef_06136_00218468711.wav\n",
       "952    /work/bbc6523/diverse_voices/british_dialects_audio/british_dialects_welsh_female/wef_06136_01071616943.wav\n",
       "Name: audio_file, dtype: object"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_df['audio_file']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "524\n",
      "50\n",
      "138\n",
      "50\n",
      "78\n",
      "50\n",
      "525\n",
      "50\n",
      "1298\n",
      "50\n",
      "1344\n",
      "50\n",
      "279\n",
      "50\n",
      "660\n",
      "50\n",
      "233\n",
      "50\n",
      "385\n",
      "50\n"
     ]
    }
   ],
   "source": [
    "all_dfs = []\n",
    "for file in subset_dfs:\n",
    "    file_df = subset_dfs[file][subset_dfs[file]['text'].isin(list(common_values_text))]\n",
    "    file_df = file_df[['line_id', 'audio_file', 'text', 'audio_duration']]\n",
    "    file_df['audio_file'] = '/work/bbc6523/diverse_voices/british_dialects_audio/' +file[:-4] + '/'+ file_df['audio_file'] + '.wav'\n",
    "    print(len(file_df))\n",
    "    file_df = file_df.drop_duplicates(subset='text')\n",
    "    print(len(file_df))\n",
    "    file_df['gender'] = file.split('_')[-1][:-4]\n",
    "    file_df['accent'] = file.split('_')[-2]\n",
    "    all_dfs.append(file_df)\n",
    "full_df = pd.concat(all_dfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_df.to_csv('british_dialects.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cultural",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
